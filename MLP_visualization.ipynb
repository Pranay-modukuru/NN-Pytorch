{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Layer Activations of MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This notebook shows a way to visualize the activations of a layer given an input to the network \n",
    " \n",
    "    - Data used to train the model is MNIST\n",
    "    - The neural network is implemented with Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from statistics import mean\n",
    "import sys\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Data \n",
    "transformer = torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root = '../data', \n",
    "                            train=True, download=True,\n",
    "                             transform=transformer)\n",
    "\n",
    "valid_dataset = torchvision.datasets.MNIST(root = '../data', \n",
    "                                train=False, download=True,\n",
    "                             transform=transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Dataloaders\n",
    "train_params = {'batch_size': 64,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 1,\n",
    "          'pin_memory' : True \n",
    "          }\n",
    "\n",
    "valid_params = {'batch_size': 1000,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 1,\n",
    "          'pin_memory' : True\n",
    "          }\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,**train_params)\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset,**valid_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run on 'cuda' if available\n",
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Class\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 100)\n",
    "        self.fc2 = nn.Linear(100,10)\n",
    "            \n",
    "    def forward(self, input):\n",
    "        input = input.view(-1, 28*28)\n",
    "        self.out1 = F.relu(self.fc1(input))\n",
    "        self.out2 = F.relu(self.fc2(self.out1))\n",
    "        return self.out2\n",
    "    \n",
    "    def execute_train(self, num_epochs, train_loader, opt, loss_fun):\n",
    "        \"\"\"\n",
    "        Trains the model for the specified number of epochs\n",
    "        \"\"\"\n",
    "        self.train()\n",
    "        losses = []\n",
    "        for epoch in range(1,num_epochs+1):\n",
    "            batch_losses = []\n",
    "            for batch_idx, (data, target) in enumerate(train_loader):\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                \n",
    "                out = self(data)\n",
    "                loss = loss_fun(out, target)\n",
    "                \n",
    "                batch_losses.append(loss.item())\n",
    "                \n",
    "                opt.zero_grad()\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "            \n",
    "            losses.append(mean(batch_losses))\n",
    "                \n",
    "            if epoch % 2 == 0:\n",
    "                print('Epoch [{}/{}] Loss : {:.4f}'.format(epoch, num_epochs, losses[epoch-1]))\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def visualize_activations(self, data, path = 'activation_images/', save_fig_name = '1', show = False):\n",
    "        \"\"\"\n",
    "        Plots input, layer activations, predicted output in one plot\n",
    "        \"\"\"\n",
    "        fig, axs = plt.subplots(1,4, figsize=(10,12))\n",
    "        for ax in axs:\n",
    "            ax.axis('off')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.forward(data)\n",
    "\n",
    "        data = data.squeeze(0).cpu().numpy()\n",
    "        self.out1 = self.out1.squeeze(0).squeeze(0).cpu().numpy().reshape(100,1)\n",
    "        self.out2 = self.out2.squeeze(0).squeeze(0).cpu().numpy().reshape(10,1)\n",
    "        actual_out = np.where(self.out2 == max(self.out2))[0][0]\n",
    "\n",
    "        axs[0].imshow(data,cmap='gist_gray',aspect=1)\n",
    "        axs[0].set_title('Input')\n",
    "        axs[1].imshow(self.out1, cmap='gist_gray',aspect=0.1)\n",
    "        axs[1].set_title('Hidden Layer activations')\n",
    "        axs[2].imshow(self.out2, cmap='gist_gray',aspect=0.09)\n",
    "        axs[2].set_title('Output Layer activations')\n",
    "        axs[3].text(0.3, 0.5, str(actual_out), fontsize=15)\n",
    "        axs[3].text(0.1, 0.6, 'Predicted', fontsize=12)\n",
    "        plt.savefig(str(path)+str(save_fig_name)+'.png')\n",
    "        if show:\n",
    "            plt.show()\n",
    "        plt.clf()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Initializing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (fc1): Linear(in_features=784, out_features=100, bias=True)\n",
       "  (fc2): Linear(in_features=100, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Network().to(device)\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Declaring the optimizer and loss function to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_sgd = torch.optim.Adam(net.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fun = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training the model\n",
    "\n",
    "    Also there is a saved version of the state dictionary of the model, which can be used if you don't want to wait for the model to be trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/50] Loss : 0.5814\n",
      "Epoch [4/50] Loss : 0.5316\n",
      "Epoch [6/50] Loss : 0.5115\n",
      "Epoch [8/50] Loss : 0.4971\n",
      "Epoch [10/50] Loss : 0.4894\n",
      "Epoch [12/50] Loss : 0.4840\n",
      "Epoch [14/50] Loss : 0.4809\n",
      "Epoch [16/50] Loss : 0.4782\n",
      "Epoch [18/50] Loss : 0.4756\n",
      "Epoch [20/50] Loss : 0.4758\n",
      "Epoch [22/50] Loss : 0.4743\n",
      "Epoch [24/50] Loss : 0.4744\n",
      "Epoch [26/50] Loss : 0.4752\n",
      "Epoch [28/50] Loss : 0.4730\n",
      "Epoch [30/50] Loss : 0.4715\n",
      "Epoch [32/50] Loss : 0.4702\n",
      "Epoch [34/50] Loss : 0.4723\n",
      "Epoch [36/50] Loss : 0.4723\n",
      "Epoch [38/50] Loss : 0.4721\n",
      "Epoch [40/50] Loss : 0.4704\n",
      "Epoch [42/50] Loss : 0.4718\n",
      "Epoch [44/50] Loss : 0.4712\n",
      "Epoch [46/50] Loss : 0.4706\n",
      "Epoch [48/50] Loss : 0.4716\n",
      "Epoch [50/50] Loss : 0.4698\n"
     ]
    }
   ],
   "source": [
    "train_losses = net.execute_train(50, train_loader, opt_sgd, loss_fun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models state dictionary i.e. weights and biases are stored into file 'model_state_dict'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'model_state_dict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If you want to skip the training\n",
    "#### Just load the state dictionary into the model\n",
    "##### but before that initialize the model with the Network class defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = Network()\n",
    "model.load_state_dict(torch.load('model_state_dict', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_data(dataset):\n",
    "    \"\"\"\n",
    "    Gets a random data from the dataset provided\n",
    "    \"\"\"\n",
    "    i = np.random.randint(100)\n",
    "    data = dataset[i][0]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calling the visualize activations method of the model and giving a random image from the validation data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x864 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.visualize_activations(get_random_data(valid_dataset), 1, show = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you are interested in showing these visualizations as a video, this part is for you!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First run the visualize method for desired number of times and make sure that these are saved in desired folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 720x864 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x864 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x864 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x864 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x864 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x864 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x864 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x864 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x864 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x864 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x864 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x864 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x864 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x864 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x864 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(15):\n",
    "    model.visualize_activations(get_random_data(valid_dataset), path = 'activation_images/', save_fig_name = i, show = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load all the images  into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import glob\n",
    "\n",
    "# path = 'path to the folder images are stored'\n",
    "path = 'activation_images'\n",
    "\n",
    "img_array = []\n",
    "for filename in glob.glob( path +'/*.png'):\n",
    "    img = cv2.imread(filename)\n",
    "    height, width, layers = img.shape\n",
    "    size = (width,height)\n",
    "    img_array.append(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### To see the number of images loaded into the list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(img_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now generate a video of desired frame rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "out = cv2.VideoWriter('activations_video.avi',cv2.VideoWriter_fourcc(*'DIVX'), 1, size)\n",
    "\n",
    "for i in range(len(img_array)):\n",
    "    out.write(img_array[i])\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"activations_video.avi\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "Video(\"activations_video.avi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
